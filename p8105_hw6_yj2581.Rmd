---
title: "p8105_hw6_yj2581"
author: "YucongJiang"
date: "2019-11-21"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
```

## Problem 1

First we build a model with all the covariates.

```{r load, message = FALSE, warning = FALSE}
#load and clean data
birthweight <- read_csv("data/birthweight.csv") %>%
  janitor::clean_names() %>%
  drop_na() %>%
  mutate(
    #factorize
    babysex = factor(babysex),
    frace = factor(frace),
    mrace = factor(mrace),
    malform = factor(malform)
  )

#build full model
model_all <- lm(bwt ~ ., data = birthweight)
summary(model_all)
```

We can see that some of the covariates have collinearities and some others are not significant. We use stepwise regression to eliminate covariates.

```{r stepwise, results = FALSE}
model_1 <- step(model_all, direction = "backward")
```


```{r plot}
summary(model_1)

#draw the plot
birthweight %>%
  add_predictions(model_1) %>%
  add_residuals(model_1) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point()+
  geom_smooth(method = "lm")
```

Therefore, we build a model from data driven process, which contains covariates including: 
babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken
The R-squared is about 0.72. We can see that the fitted line of residuals against predictions is 0, showing that the assumption of normality is acceptable.

Then we build another two model.

```{r model_2_3}
#cross validation
cv_df <- birthweight %>%
  crossv_mc(n = 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>%
  mutate(
    model_1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .)),
    model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .)),
    model_3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .))
  ) %>%
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))
  )

#violin plot
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We can see that our model "1" has the least rmse, which means that it has the least expected error when predicting the birthweight, compared with the other two models.

Model 2 has the highest rmse.

## Problem 2

First read the data.

```{r load_2, message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Then create bootstrap samples

```{r bootstrap}
boot_strap <- weather_df %>%
  bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .))
  )
```

Calculate the two value and show their distributions.

```{r b0b1}
boot_res <- boot_strap %>%
  mutate(
    results = map(models, broom::tidy)
  ) %>%
  select(results) %>% unnest(results)

#Use filter to split beta_0 and beta_1, then join
beta_0 <- boot_res %>%
  filter(term == "(Intercept)") %>% select(estimate) %>% rename(beta_0 = estimate)
beta_1 <- boot_res %>%
  filter(term == "tmin") %>% select(estimate) %>% rename(beta_1 = estimate)

beta_0_1 <- cbind(beta_0, beta_1) %>% 
  mutate(
    log_product = log(beta_0 * beta_1)
  ) %>% select(log_product)

ggplot(beta_0_1, aes(x = log_product)) + geom_density()
```

```{r r2}
#Use broom::glance to extract r^2
r_sqr <- boot_strap %>%
  mutate(
    glance = map(models, broom::glance)
  ) %>%
  select(glance) %>% unnest(glance) %>% select(r.squared)

ggplot(r_sqr, aes(x = r.squared)) + geom_density()
```

From the two plot above, we can see that the distribution of both two values are close to normal.

```{r CI}
CI_beta <- quantile(pull(beta_0_1, log_product), c(0.025, 0.975))
CI_r2 <- quantile(pull(r_sqr, r.squared), c(0.025, 0.975))
```

The 95% CI of `log(beta_0 * beta_1)` is (`r CI_beta[[1]]`, `r CI_beta[[2]]`);
the 95% CI of `R^2` is (`r CI_r2[[1]]`, `r CI_r2[[2]]`).


